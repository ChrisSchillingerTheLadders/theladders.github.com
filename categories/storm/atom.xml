<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Storm | TheLadders Engineering Stories]]></title>
  <link href="http://dev.theladders.com/categories/storm/atom.xml" rel="self"/>
  <link href="http://dev.theladders.com/"/>
  <updated>2015-04-27T10:42:48-04:00</updated>
  <id>http://dev.theladders.com/</id>
  <author>
    <name><![CDATA[TheLadders Engineering]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Writing better API tests]]></title>
    <link href="http://dev.theladders.com/2015/04/writing-better-api-tests/"/>
    <updated>2015-04-27T17:41:00-04:00</updated>
    <id>http://dev.theladders.com/2015/04/writing-better-api-tests</id>
    <content type="html"><![CDATA[<p><blockquote><p>It is only through labor and painful effort, by grim energy and resolute courage, that we move on to better things.</p><footer><strong>&mdash;Theodore Roosevelt</strong></footer></blockquote></p>

<h1>The problem</h1>

<p>When I was first tasked with writing the web service for our <a href="http://www.theladders.com/mobile/">jobseeker iOS app</a>, I wanted to be able to write tests against the API without diving into the world of iOS automated functional testing.  I really didn’t see a need to fire up an iOS client just so I could make calls to a service that was under heavy development and in a constant state of change.  So how could I write tests that would exercise the API in a way similar to an iOS client?</p>

<p>Fortunately for me, engineers at TheLadders have always emphasized testing in one form or another.  Sometimes this means a unit test for a particular method on a class.  Other times it means an automated functional test (AFT) verifying client/service interactions for a particular feature.  And in some instances it may even mean, f**k it, ship it and see if it blows up in production (a rare occurrence of course).</p>

<p>It turns out that before I even started working on the mobile service, people before me were asking similar questions about testing their APIs, and they had come up with a slightly different type of test that we now call &ldquo;web tests&rdquo;.  A &ldquo;web test&rdquo; is basically a way to fire up an instance of the service for each test and, using a Jersey client, programmatically make calls to that service within the test itself.  This was great because it allowed us to write tests against our APIs without needing to fire up an external tool such as a web browser.  The following code is an example of such a test:</p>

<p><code>java Basic web test
TestRestClient client = createTestRestClient();
Response response = client.get(“api/jobs”);
assertEquals(200, response.getStatus());
</code></p>

<p>This code makes a call to retrieve a list of jobs and verifies the response status code is 200.</p>

<p>The idea of the “web” or “API test” was exactly what I needed.  This gave me a starting point.  But I knew these web tests could be improved upon, making it easier to test our APIs.  So I set about trying to figure out what I wanted for testing the mobile service and came up with a list of requirements.</p>

<h1>The requirements</h1>

<p>There were some requirements I wanted to establish that would help guide my decision-making process when writing web tests.  These requirements included the following:</p>

<ul>
<li>The tests should mimic how an iOS client would navigate the API</li>
<li>Anyone reading the test code should be able to easily understand what is being tested</li>
<li>The code should be easy to reuse, reducing duplication where possible</li>
</ul>


<p>These really aren’t groundbreaking ideas, but they were helpful to keep in mind at all times.  They served as guideposts throughout my decision-making process.  With these requirements defined, I&rsquo;m going to give you a quick rundown of some key elements of the API before diving into implementation details.</p>

<h1>The API</h1>

<p>The API for the mobile service is a RESTful API that uses <a href="http://en.wikipedia.org/wiki/HATEOAS">hypermedia as the engine of application state (HATEOAS)</a>.  We use the HTTP methods GET, DELETE, POST, and PUT against URLs representing resources.  Everything is stateless and we leverage headers for things such as authorization and versioning.  The following figure illustrates sample request and response against this API:</p>

<p><img class="center medium" src="/images/api-tests/api-request-response.png"></p>

<p>The above example illustrates a request for the details of an individual job.  One important thing to note here is the “links” element in the JSON.  This is the HATEOAS piece I mentioned earlier. If a user taps the “like” button on the app for this particular job, the client code would send a request to the URL associated with the “like” action.  If the job was already liked when it was retrieved, then an “unlike” link would have been present instead.</p>

<p>We use hypermedia links extensively in the API.  In fact, the only URL that is hard-coded in the iOS application is what we call a “bootstrap” URL.  This URL gives the client all of the hypermedia links needed to get started with the app, and would look something like the following:</p>

<p>``` json Bootstrap JSON response
{
  &ldquo;links&rdquo; : [{</p>

<pre><code>"action" : "authenticate",
"method" : "POST",
"uri" : "https://localhost:54837/authenticate",
</code></pre>

<p>  }, {</p>

<pre><code>"action" : "lookupIndustries",
"method" : "GET",
"uri" : "https://localhost:54837/api/lookup/industries",
</code></pre>

<p>  }, {</p>

<pre><code>"action" : "autocompleteTitle",
"method" : "GET",
"uri" : "https://localhost:54837/api/autocomplete/title?q={toMatch}",
</code></pre>

<p>  }, {</p>

<pre><code>"action" : "register",
"method" : "POST",
"uri" : "https://localhost:54837/register",
</code></pre>

<p>  }]
}
```</p>

<p>From here, the client has the links for retrieving some lookup lists, maybe a URL for auto-completing the job title field, a link for logging in an existing user, and a link for registering a new user.  If the link to login is invoked, then new response JSON will be returned containing “links” for valid actions that user can perform, and so on and so forth.</p>

<p>There is more to the API and I’ve glossed over some details.  But this should give you enough of an idea of how things work to understand the implementation.</p>

<h1>The implementation</h1>

<p>It’s important to keep in mind that the following implementation has evolved over time.  What I present to you here is an example that identifies the key components of the current state.  I was always going back-and-forth with different ideas, trying out various implementations.  And I’m pretty certain the team will continue to iterate on what we have today.</p>

<p>But in its current state, the implementation has the following four components:</p>

<ol>
<li>Class representing request/response JSON</li>
<li>Class for performing asserts against a body of JSON</li>
<li>Class representing the actions that can be taken on a body of JSON (such as calling hypermedia links or accessing an item in a list)</li>
<li>Class representing a user, their state, and their “connection” to the API.</li>
</ol>


<p>These four components and how they relate to the API can be seen in the following figure:</p>

<p><img class="center medium" src="/images/api-tests/api-implementation-components.png"></p>

<p>In addition to these four components, I did my  best to provide some “syntactic sugar” for allowing us to write tests in the given/when/then style.  So let’s dive into each of the four components and give you some concrete examples to work with.</p>

<h2>Class representing request/response JSON</h2>

<p>We have what are called “representation” classes whose sole purpose is to “represent” incoming or outgoing JSON.  These classes are annotated with JAXB annotations for serialization/deserialization purposes.  These classes also have methods for accessing the hypermedia links (some of which may be optional).  So the following JSON:</p>

<p>``` json Job JSON
{
  &ldquo;details&rdquo; : {</p>

<pre><code>"title" : "Software Engineer", 
"company" : "TheLadders", 
"location" : "New York, NY", 
"postedOn" : "2015-04-17", 
"salary" : "$100k", 
"description" : "Work with us!" 
</code></pre>

<p>  },
  &ldquo;links&rdquo; : [ {</p>

<pre><code>"action" : "like",
"method" : "POST", 
"uri" : "https://localhost:54837/api/jobs/e-1/like" 
</code></pre>

<p>  }, {</p>

<pre><code>"action" : "get",
"method" : "GET", 
"uri" : "https://localhost:54837/api/jobs/e-1" 
</code></pre>

<p>  } ]
}
```</p>

<p>Would be represented by a class that looks like:</p>

<p>``` java JobRepresentation.java
@XmlRootElement
@XmlAccessorType(XmlAccessType.FIELD)
public class JobRepresentation
{
  public JobDetailRepresentation details;
  public HypermediaLinks links;</p>

<p>  &hellip;</p>

<p>  public Optional<HypermediaLink> likeLink()
  {</p>

<pre><code>return Optional.ofNullable(links.forAction(“like”));
</code></pre>

<p>  }</p>

<p>  public Optional<HypermediaLink> unlikeLink()
  {</p>

<pre><code>return Optional.ofNullable(links.forAction(“unlike”));
</code></pre>

<p>  }</p>

<p>  public HypermediaLink getLink()
  {</p>

<pre><code>return links.forAction(“get”);
</code></pre>

<p>  }
}
```</p>

<p>The methods for the links are particularly useful when navigating the API in a way an actual client would in tests.  Having defined a class representing a body of JSON, let&rsquo;s move on to the class for performing assertions against this body of JSON.</p>

<h2>Class for performing asserts against a body of JSON</h2>

<p>Having a class for performing assertions against a particular body of JSON helps us achieve two goals:  1) write more expressive test code and 2) have assertions that can be reused across tests.  For example, there may be several tests that require asserting the title for a particular job returned by the API.  We may also want to verify the presence or absence of certain hypermedia links.  The following code illustrates an “asserts” class for a job representation.</p>

<p>``` java JobRepresentationAsserts.java
public class JobRepresentationAsserts
{
  private final JobRepresentation representation;</p>

<p>  &hellip;</p>

<p>  public void hasTitleOf(String title)
  {</p>

<pre><code>assertEquals(title, representation.details.title);
</code></pre>

<p>  }</p>

<p>  &hellip;</p>

<p>  public void hasLikeLink()
  {</p>

<pre><code>assertTrue(representation.likeLink().isPresent());
</code></pre>

<p>  }</p>

<p>  public void doesNotHaveLikeLink()
  {</p>

<pre><code>assertFalse(representation.likeLink().isPresent());
</code></pre>

<p>  }</p>

<p>  public void hasUnlikeLink()
  {</p>

<pre><code>assertTrue(representation.unlikeLink().isPresent());
</code></pre>

<p>  }</p>

<p>  public void doesNotHaveUnlikeLink()
  {</p>

<pre><code>assertFalse(representation.unlikeLink().isPresent());
</code></pre>

<p>  }
}
```</p>

<p>In addition to these “asserts” classes, I created a class with static builder methods to provide some syntactic sugar.  This allows us to write our tests in a given/when/then fashion.  This builder class is as follows:</p>

<p>``` java AssertsBuilder.java
public class AssertsBuilder
{
  &hellip;</p>

<p>  public static JobRepresentationAsserts thenThe(JobRepresentation representation)
  {</p>

<pre><code>return new JobRepresentationAsserts(representation);
</code></pre>

<p>  }</p>

<p>  &hellip;
}
```</p>

<p>This results in something like the following in a test:</p>

<p>``` java Sample job test
import static com.theladders.AssertsBuilder.*;</p>

<p>&hellip;</p>

<p>JobRepresentation job = apiCallToGetJob();
thenThe(job).hasTitleOf(“Software Engineer”);
thenThe(job).hasLikeLink();
```</p>

<p>We’re not there yet, but we’re making progress.  So far we have the following:</p>

<ul>
<li>Class for representing incoming/outgoing bodies of JSON</li>
<li>Class for performing assertions against a particular body of JSON</li>
<li>Some syntactic sugar for creating assert-related objects.</li>
</ul>


<p>The next piece of the puzzle is a class representing the actions that can be taken on a body of JSON.</p>

<h2>Class representing the actions that can be taken on a body of JSON</h2>

<p>What exactly do I mean by &ldquo;taking actions on a body of JSON&rdquo;.  &ldquo;Actions&rdquo; can include anything from selecting an individual item in a list (a job in a list of jobs) to calling a URL associated with a hypermedia link.  So why the need for a separate class encapsulating these &ldquo;actions&rdquo; when we already have a representation class for a body of JSON (which includes hypermedia links)?  The reason is rooted in the fact that our representation classes are used in production code for serialization purposes (a topic worthy of its own blog post), and I didn&rsquo;t want to mix in testing concerns with our representation classes for something like calling the API via TestRestClient.  So I decided to create a different type of class for this purpose.  I called this the &ldquo;actions&rdquo; class.</p>

<p>Let&rsquo;s take a look at JSON containing a list of jobs as an example.  If a user is viewing a list of jobs, it&rsquo;s reasonable to expect they might want to &ldquo;do something&rdquo; with a particular job.  This would be captured with something like the following:</p>

<p>``` java JobListActions.java
public class JobListActions
{
  public UserAndClient userAndClient; // this class represents a user and their connection to the API and will be discussed next.
  public JobListRepresentation representation;</p>

<p>  &hellip;</p>

<p>  public JobActions actingOnJobAtIndex(int index)
  {</p>

<pre><code>return new JobActions(userAndClient, representation.jobs.get(index));
</code></pre>

<p>  }
}
```</p>

<p>JobListActions by itself isn’t necessarily interesting, but it does provide a nice way for indicating what job a user is acting upon in a test.  The next code listing is more interesting, providing methods for performing various “actions” an individual job.</p>

<p>``` java JobActions.java
public class JobActions
{
  private final UserAndClient userAndClient; // this class represents a user and their connection to the API and will be discussed next
  private final JobRepresentation representation;</p>

<p>  &hellip;</p>

<p>  public Response<JobRepresentation> like()
  {</p>

<pre><code>Optional&lt;HypermediaLink&gt; link = representation.likeLink();
return new Response&lt;&gt;(userAndClient.client.post(link.get()), JobRepresentation.class);
</code></pre>

<p>  }</p>

<p>  public Response<JobRepresentation> unlike()
  {</p>

<pre><code>Optional&lt;HypermediaLink&gt; link = representation.unlikeLink();
return new Response&lt;&gt;(userAndClient.client.post(link.get()), JobRepresentation.class);
</code></pre>

<p>  }
}
```</p>

<p>This class contains methods for liking and un-liking a job.  This is done by using the TestRestClient found in UserAndClient (more on this next) to make calls to the API via the hypermedia links.  The Response class that is returned from these methods is a simple wrapper class for a JAX-RS response and looks like the following:</p>

<p>``` java Response.java
public class Response<T>
{
  private final javax.ws.rs.core.Response response;
  private final Class<T> responseBodyType;</p>

<p>  &hellip;</p>

<p>  public int status()
  {</p>

<pre><code>return response.getStatus();
</code></pre>

<p>  }</p>

<p>  public T body()
  {</p>

<pre><code>return response.readEntity(responseBodyType);
</code></pre>

<p>  }
}
```</p>

<p>So we have all of these separate pieces:  1) representation class, 2) representation asserts class, and 3) class for performing actions against a representation.  By themselves, these three pieces really aren’t that useful.  We need something to pull them together.  This is where the UserAndClient alluded to earlier comes in.</p>

<h2>Class representing a user, their state, and their “connection” to the API</h2>

<p>This class is used for encapsulating a user and how they make calls to the API.  State for a particular user is kept here.  This class acts as the launching point for navigating through the API.  The code in the following listing provides methods for viewing job matches and being able to act upon the viewed job matches.</p>

<p>``` java UserAndClient.java
public class UserAndClient
{
  public AuthenticatedUserRepresentation user;
  public TestRestClient client;</p>

<p>  &hellip;</p>

<p>  public JobListRepresentation jobMatches;</p>

<p>  &hellip;</p>

<p>  public JobListRepresentation viewsJobMatches()
  {</p>

<pre><code>HypermediaLink link = user.jobMatchesLink();
jobMatches = client.post(link).readEntity(JobListRepresentation.class);
return jobMatches;
</code></pre>

<p>  }</p>

<p>  public JobListActions actingOn(JobListRepresentation representation)
  {</p>

<pre><code>return new JobListAction(this, representation);
</code></pre>

<p>  }</p>

<p>  &hellip;
}
```</p>

<p>With some additional syntactic sugar, we are able to use UserAndClient to write very expressive tests with decoupled and reusable components.  Our base test class provides this syntactic sugar:</p>

<p>``` java RegisteredUserWebTest.java
public class RegisteredUserWebTest extends WebTest
{
  protected UserAndClient me;</p>

<p>  @Before
  public void setUp()
  {</p>

<pre><code>me = createNewUserAndClient();
</code></pre>

<p>  }</p>

<p>  &hellip;</p>

<p>  public UserAndClient given(UserAndClient userAndClient)
  {</p>

<pre><code>return userAndClient;
</code></pre>

<p>  }</p>

<p>  public UserAndClient when(UserAndClient userAndClient)
  {</p>

<pre><code>return userAndClient;
</code></pre>

<p>  }
}
```</p>

<p>We are then able to put it all together in a test such as the following:</p>

<p>``` java LikeJobWebTest.java
public class LikeJobWebTest extends RegisteredUserWebTest
{
  @Test
  public void likingJobReturnsResultWithUnlikeLink()
  {</p>

<pre><code>given(me).viewsJobMatches();
Response&lt;JobRepresentation&gt; response = when(me).actingOn(me.jobMatches)
                                               .actingOnJobAtIndex(0)
                                               .like();
thenThe(response).hasStatusOfOk();

JobRepresentation job = response.body();
thenThe(job).doesNotHaveLikeLink();
thenThe(job).hasUnlikeLink();
</code></pre>

<p>  }
}
```</p>

<p>Looking at the above test, it’s pretty clear what is happening.  A user views their job matches.  From there, they like a single job in the list.  We are then verifying that the response of that like contains JSON without a like link (so they cannot “like” a job two times in a row).</p>

<h1>Conclusion</h1>

<p>Overall the results have been positive.  We have fairly reusable test code that minimizes duplication.  At first glance it may seem like a lot of unnecessary classes (representation asserts &amp; a class for encapsulating “actions”).  But creating these classes forces us to really think about the API as we develop.  An “asserts” class forces us to zero in on the needs of the client.  The “actions” class gives us a feel for whether or not we are providing the best API in terms of hypermedia links and navigating between URLs.</p>

<p>At the end of the day, we wanted something that would result in an API that met the needs of it’s customer (the iOS client), and I think we’ve achieved that.  By “eating our own dog food” via navigating the hypermedia links in our web tests, we are putting ourselves in the shoes of the iOS client, and the result has been increased test coverage with tests that are easy to extend and understand.</p>

<p>Hopefully you have found this useful.  I realize I&rsquo;ve glossed over some of the finer details, such as &ldquo;what is actually inside of the TestRestClient&rdquo; class?  This blog post wasn&rsquo;t meant to give you the exact template for doing what we do here at TheLadders.  It was meant to get you thinking about how you are testing your APIs and possible ways to improve upon how that is done.  I&rsquo;m sure what we have today will look much different six months for now.  While not perfect, it has resulted in what I think are more stable APIs, and at the end of the day, that&rsquo;s a win in my book.</p>

<p>Now time to go test that new Storm topology of mine in production and see if it works&hellip;</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Thunder and Lightning]]></title>
    <link href="http://dev.theladders.com/2014/07/thunder-and-lightning/"/>
    <updated>2014-07-23T09:00:00-04:00</updated>
    <id>http://dev.theladders.com/2014/07/thunder-and-lightning</id>
    <content type="html"><![CDATA[<p><blockquote><p>The simplest of explanations is more likely to be correct than any other.</p><footer><strong>&mdash;Occam&rsquo;s Razor</strong></footer></blockquote></p>

<h2>Braving the Storm</h2>

<p>At TheLadders, we operate a fully virtualized environment consisting of slightly less than a thousand virtual machines across our Development, QA, and Production environments.  We manage these systems with <a href="http://puppetlabs.com">Puppet</a> and <a href="http://theforeman.org">Foreman</a>, which enables us to rapidly deploy new systems when necessary, as well as maintain our systems predictably throughout their lifecycles.</p>

<p>One of the tools we <a href="http://dev.theladders.com/categories/storm/">rely on heavily</a> is <a href="https://storm.incubator.apache.org">Storm</a>, a distributed, real-time computation system.  Storm provides us with a framework that we use to constantly crunch data about the millions of job seekers and jobs in our environment.  This enables us to provide our job seekers with relevant information about the best jobs available to them at any given time.  When we build new Storm nodes, Puppet takes a very minimal OS install, lays down our standard configuration, then installs Storm, starts the Storm process and ensures it will start after reboot.</p>

<h2>Dark Skies Ahead</h2>

<p>We operate several Storm clusters across QA and Production spanning Storm versions 0.8 and 0.9.  Over the past several months, we&rsquo;ve experienced intermittent issues within the clusters where individual nodes stopped behaving properly.  The issues occur more frequently in our Production environment, which we attribute to the orders of magnitude higher volumes of traffic traversing Production.  We&rsquo;ve also seen this particular issue in both our 0.8 and 0.9 clusters.  Until recently, the problem has occurred so infrequently that it was much quicker and easier to shut down and rebuild the problem nodes than invest significant time nailing down the root cause.</p>

<p>Last month, we rebuilt our 0.9 Production cluster from the ground up and immediately began seeing topologies fail to start on multiple workers.  The issue was clouded by the fact that we saw several different errors occurring, including too many files open, DNS resolution failures, Java class not found errors, heartbeat file not found, etc.</p>

<p><span class='caption-wrapper center medium'><img class='caption' src='/images/thunder-and-lightning/stormy-city.jpg' width='' height='' alt='<a href="https://www.flickr.com/photos/29311691@N05/7653430352">Photo</a> by <a href="https://www.flickr.com/photos/29311691@N05/">H.L.I.T.</a> / <a href="http://creativecommons.org/licenses/by/2.0/">CC BY 2.0</a>' title='<a href="https://www.flickr.com/photos/29311691@N05/7653430352">Photo</a> by <a href="https://www.flickr.com/photos/29311691@N05/">H.L.I.T.</a> / <a href="http://creativecommons.org/licenses/by/2.0/">CC BY 2.0</a>'><span class='caption-text'><a href="https://www.flickr.com/photos/29311691@N05/7653430352">Photo</a> by <a href="https://www.flickr.com/photos/29311691@N05/">H.L.I.T.</a> / <a href="http://creativecommons.org/licenses/by/2.0/">CC BY 2.0</a></span></span></p>

<h2>When it Rains, it Pours</h2>

<p>Since we were seeing multiple errors occurring without any obviously predictable pattern, we started the investigation by trying to reproduce or verify the individual errors on the nodes where we saw the issues in the Nimbus interface.  Despite complaints from Storm of DNS resolution issues, we were unable to find any issues with our DNS system or name resolution on any of the nodes in the cluster, even when performing many lookups in rapid succession.</p>

<p>After eliminating DNS as a root cause, we surmised that the real problem was limits on open file handles and that the other errors &mdash; Java class not found, heartbeat file not found and DNS resolution failure &mdash; were just different manifestations of the process’ inability to open a file handle or socket.  One change that Puppet makes to our systems is to increase the open file handle limits for the storm user/process from the default of 1024 to 256k.  We do this by setting the <code>nofile</code> option in <code>/etc/security/limits.conf</code>.  We verified on every host that the Storm user had properly set file handle limits.  Observing the workers when they were experiencing the issue proved difficult because Storm dynamically assigns workers to nodes at process startup and processes are not sticky.  This means that in our situation, where processes were starting and dying very quickly, it was extremely challenging to be logged into the host watching the process and gathering useful data in the few seconds between startup and death.  One approach to avoid this problem was to shut down workers to eliminate unused worker slots, thus limiting the potential destinations for new processes.  After a prolonged struggle with observing a process as it died, we were finally able to see that the worker process itself was limited to the default 1024 file handles.  We confirmed this suspicion by watching <code>/proc/&lt;PID&gt;/limits</code> to confirm that all Storm related processes were limited to 1024 open file handles on the affected hosts.</p>

<h2>Every Cloud has a Silver Lining</h2>

<p>Now that we had observed a worker process with a 1024 open file handle limit, we moved on to determining how this could happen and why it seemed to occur only on certain nodes.  We noted that rebooting a host did not resolve the issue and further that rebooting a working node caused it to cease functioning properly.  After quite a bit of experimentation, we found that manually restarting the Storm supervisor on an affected host allowed the node to function properly again, at least until the next reboot.</p>

<p>We recently altered our new machine deployment to reboot the host between running Puppet and putting the machine in service.  Whereas previously the Storm supervisor would be started by Puppet and function normally, the supervisor is now being started by init on boot.</p>

<p>We ultimately determined that the root cause of this issue is that processes started by init don&rsquo;t go through pam, so limits set in <code>/etc/security/limits.conf</code>, which is utilized by <code>pam_limits.so</code>, are not applied to processes started on boot.</p>

<p>We chose to solve this issue by following the RHEL convention of configuration in /etc/sysconfig and modifying the storm supervisor init script to load <code>/etc/sysconfig/storm</code> if it exists.  Our <code>/etc/sysconfig/storm</code> contains a single line for the time being, increasing the <code>nofile</code> limit to 256k.  This method provides us with the flexibility to augment the configuration in the future with minimal impact.</p>

<p>Once Puppet deployed this change to our entire environment, we verified via <code>/proc/&lt;PID&gt;/limits</code> that the Storm supervisors had picked up the changes both when started by hand/Puppet and when started on boot.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Brewing Storm]]></title>
    <link href="http://dev.theladders.com/2013/12/a-brewing-storm/"/>
    <updated>2013-12-19T11:45:00-05:00</updated>
    <id>http://dev.theladders.com/2013/12/a-brewing-storm</id>
    <content type="html"><![CDATA[<p><blockquote><p>If there&rsquo;s a book that you want to read, but it hasn&rsquo;t been written yet, then you must write it</p><footer><strong>&mdash;Toni Morrison</strong></footer></blockquote>
Once upon a time there was a script. Each morning it sent new jobs to our job seeking customers. It was simple and the data it operated on was relatively small.  As the months turned into years this script grew in complexity and the data grew in size.  And more scripts came to the party and they had data of their own and these new scripts beget data for other scripts, and this small group of scripts extracted and transformed, processed and prepared, working tirelessly every night.</p>

<p>Some worked alone while others fed each other, all growing increasingly more complex.  Occasionally at first and more frequently over time, the scripts started to interfere with each other by competing for resources or not finishing in time for a child script to consume it’s parents data, or worst of all, deadlocking for no apparent reason.</p>

<p>Let’s bring some order we said and used cron to wrangle them at first, then moving to more complex “enterprise” scheduling systems to try and tame the beast, and keep the scripts from clobbering each other or grinding themselves to a crawl.</p>

<p>But the data grew and grew and with it came longer and longer run times.  At this same time our script was getting slower, our users’ expectations and our own ambitions grew.  Could we alert a job seeker the minute a new relevant job entered our system?  Could we alert an employer as soon as job seeker they were interested in updated their profile?</p>

<p>The network of interactions/activity/entities we wanted to grow, monitor and react to and then extract value from was exploding in complexity.  You can picture all of our job seekers, employers and jobs as nodes in a graph, and imagine our teams are furiously trying to connect them in new and interesting ways.  They are using machine learning to light up edges between these nodes, indicating which jobs a particular job seeker might be interested in and which job seekers an employer might like.  Or clustering these entities into similar groups.</p>

<p>Doing all of this between 12am and 6am was getting hard. Hadoop was an option, but processing speed alone wasn’t just the issue.  Some things we wanted to do had to be done in real time, not batch.  A user isn’t going to wait until the nightly run for us to calculate what jobs are appropriate for them.  In addtion, as TheLadders moves into mobile and speed becomes more and more a concern we want the Graph of our Ecosystem to be as current as possible not something that gets updated once a day.</p>

<p>There comes this thing. Storm. It fits into our infrastructure; working nicely with RabbitMQ and interop’ing well with our existing code base.</p>

<p>However, Storm is young and fresh out of Twitter.  Getting it to work, quickly and reliably can be painful as can figuring out the best practices to manage it.  All of this took us some time and some hard lessons had to be learnt.  A lot of late nights and head scratching, time spent hanging out in irc channels, reading blog posts, watching conference talks and just plain ole trial and error and occassionally grabbing Nathan Marz when he was speaking to ask “How do we deal with X?” brought us to our current place.</p>

<p>Today Storm runs most of the backend at TheLadders, keeping that graph of our ecosystem fresh so our experience teams can ship great new features to our customers.  And we hope you can find a use for Storm and maybe get there quicker than we did.  Manning publications reached out to us and I’m incredibly excited and very proud to announce that three of our top engineers, Sean Allen, Matt Jankowski and Peter Pathirana produced a book, Storm Applied, a book designed to help you leverage some of the hard earned knowledge we’ve acquired here at TheLadders. Storm Applied will be available in the <a href="http://www.manning.com/sallen/">Manning Early Access Program</a> any day now.  We hope you find it useful and we look forward to your feedback on the book.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Denormalize the Datas for Great Good]]></title>
    <link href="http://dev.theladders.com/2013/07/denormalize-the-datas-for-great-good/"/>
    <updated>2013-07-08T16:02:00-04:00</updated>
    <id>http://dev.theladders.com/2013/07/denormalize-the-datas-for-great-good</id>
    <content type="html"><![CDATA[<p><blockquote><p>Normal is not something to aspire to, it&rsquo;s something to get away from.</p><footer><strong>&mdash;Jodie Foster</strong></footer></blockquote></p>

<h2>Scout reads go slow</h2>

<p>A few weeks ago, as we were about to launch our <a href="http://app.appsflyer.com/id654867487?pid=TLC_organic">iPhone app</a>, we discovered that one of its core features, Scout, frequently took seconds to render.</p>

<center>
<span class='caption-wrapper small'><img class='caption' src='/images/denormalize-the-datas-for-great-good/scout-screenshot.png' width='' height='' alt='Scout' title='Scout'><span class='caption-text'>Scout</span></span>
</center>


<p>For a little background as to what Scout is, at TheLadders our mission is to find the right person for the right job. One of the ways we strive to deliver on that promise is to provide jobseekers information about jobs they’ll find nowhere else. Serving that mission is Scout, which in a nutshell allows jobseekers to view anonymized information about applicants who have applied to the job they are viewing. Salary, education, career history: we present a lot of useful information to jobseekers about their competition for any given job.</p>

<p>Over time, some attractive jobs accumulate on the order of 30 to 60 applicants, yielding response times of over 1 second (due to multiple synchronous requests, done serially, just to serve <em>one</em> Scout view request).  In cases of higher load, sometimes request times take well over that.</p>

<center>
<span class='caption-wrapper small'><img class='caption' src='/images/denormalize-the-datas-for-great-good/scout-screenshot-many-applies.png' width='' height='' alt='Scout view of a job with many applicants' title='Scout view of a job with many applicants'><span class='caption-text'>Scout view of a job with many applicants</span></span>
</center>


<p>That brings Scout into unusably slow country, as the Graphite chart below indicates:</p>

<center>
<span class='caption-wrapper medium'><img class='caption' src='/images/denormalize-the-datas-for-great-good/before-graphite.png' width='' height='' alt='95th percentile of response times for Scout in seconds' title='95th percentile of response times for Scout in seconds'><span class='caption-text'>95th percentile of response times for Scout in seconds</span></span>
</center>


<p>The graph shows the time it takes to form a response to a view-job request issued by our iPhone app. It’s the 95th percentile, which means that 5% of requests had times of the lines in the graph or higher for any given date.  One in twenty requests took this long or longer. There are many lines because we have a horizontally scalable architecture, so there are many backend app nodes.</p>

<p>We managed to bring those seconds down to milliseconds, with about a 1000x decrease in times of high load.  Below I’ll describe the changes in our architecture that enabled us to make such a huge improvement.</p>

<hr />

<h2>Architecture</h2>

<p>In its initial implementation, Scout’s applicant information was gathered and assembled on the fly for each and every request. Driving the iPhone app, we have a backend app server, which is essentially just a number of RESTful endpoints against which our iPhone app issues requests.  Below is a quick rundown of the architecture before I trace a request through our architecture.</p>

<center>
<span class='caption-wrapper medium'><img class='caption' src='/images/denormalize-the-datas-for-great-good/front-end-orchestration.png' width='' height='' alt='iPhone app talks to the backend app server' title='iPhone app talks to the backend app server'><span class='caption-text'>iPhone app talks to the backend app server</span></span>
</center>


<p>Below this backend server there are a number of RESTful entity servers with which the app server is interacting via HTTP.</p>

<p><span class='caption-wrapper center medium'><img class='caption' src='/images/denormalize-the-datas-for-great-good/front-end-orchestration-entity.png' width='' height='' alt='Backend app server relies on entity servers' title='Backend app server relies on entity servers'><span class='caption-text'>Backend app server relies on entity servers</span></span></p>

<p>These entity servers in turn query each other and the canonical data store, in our case Clustrix, and that’s that.</p>

<p><span class='caption-wrapper center medium'><img class='caption' src='/images/denormalize-the-datas-for-great-good/front-end-orchestration-entity-clustrix.png' width='' height='' alt='Entity servers query the db' title='Entity servers query the db'><span class='caption-text'>Entity servers query the db</span></span></p>

<p>So when a user of our iPhone app taps on a job, a request is sent to the backend app server&hellip;</p>

<p><img class="center medium" src="/images/denormalize-the-datas-for-great-good/mobile-orchestration-request.png" title="iPhone app makes a request" ></p>

<p>&hellip;which then issues a request to our job application service for all job applications for that job. The response contains a number of links to the where those job applications may be retrieved.</p>

<p><img class="center medium" src="/images/denormalize-the-datas-for-great-good/mobile-orchestration-service-request.png" title="backend server queries the job application service for all applications to a job" ></p>

<p>The backend server iterates over those links, requesting the job applications themselves one at a time. Just as before, adhering to hypermedia design, the response contains a link to the jobseeker who applied to the job. For your sanity, I’ve simplified the response to contain only the job seeker link:</p>

<p><img class="center medium" src="/images/denormalize-the-datas-for-great-good/mobile-orchestration-service-request2.png" title="backend retrieves each application" ></p>

<p>Finally with that result set, the orchestration service then issues a number of requests to the job seeker service for information about the job seekers who have applied to the job being viewed.  In its initial implementation all of the requests were synchronous and in series as I mentioned earlier. We eventually parallelized them, as you can see in the Graphite chart where the big spikes left diminish towards the right.</p>

<p><img class="center medium" src="/images/denormalize-the-datas-for-great-good/mobile-orchestration-service-request3.png" title="backend retrieves each application" ></p>

<p>The iPhone app backend server then extracts the relevant information from those job seekers’ profiles, and returns them as a JSON array of applicants to the mobile app.</p>

<p><img class="center medium" src="/images/denormalize-the-datas-for-great-good/mobile-orchestration-response.png" title="backend retrieves each application" ></p>

<p>That is not just a lot of words and diagrams, that is a lot of work!</p>

<p>The workflow includes multiple objects serializing and deserializing, HTTP transfers, hitting the canonical store etc. Why does each request need to assemble this data itself? Why bother hitting the database? Is there an alternative? It seems like a natural fit for a document-oriented database, as the data we are passing back to the client is just a JSON object containing an array of applicants.  We could stand a <a href="http://dev.theladders.com/2013/05/varnish-in-five-acts/">Varnish cache</a> in front of the Scout endpoints on the orchestration service, but then we’d be trading freshness for speed. On the platform team we like to deliver data fast and fresh (and furious).</p>

<p><img class="center" src="/images/denormalize-the-datas-for-great-good/tokyo-drift-o.gif" title="how we roll at the Democratic Republic of Platformia" ></p>

<hr />

<h2>Scout reads go fast</h2>

<p>Principal Architect <a href="http://twitter.com/SeanTAllen">Sean T Allen</a> set <a href="http://twitter.com/casio_juarez">Andy Turley</a> and me to improving Scout’s performance. The architecture is surprisingly simple: stick the data in <a href="http://www.couchbase.com/">Couchbase</a> and have the iPhone app backend query that instead. How would we keep this data up-to-date? The first step is to have the job application entity service emit a RabbitMQ event when it receives an application from a job seeker to a particular job (a PUT returning a 201).  On the other end of that message queue there is a  <a href="http://dev.theladders.com/2013/03/riders-on-the-storm-take-a-long-holiday-let-your-children-play/">Storm</a> topology that should listen for that message. The RabbitMQ message would be the entry point into the spout.</p>

<p>The message contains a link to the job seeker who applied to the job, as well as the ID for the job to which she applied.   The message isn’t actually encoded as JSON and transmitted over the wire, but for clarity I’ve displayed the RabbitMQ message as JSON.</p>

<p><img class="center" src="/images/denormalize-the-datas-for-great-good/rabbitmq-storm.png" title="RabbitMQ passes along a job-application message to a listening Storm topology" ></p>

<p>The second step, after having received the RabbitMQ message, fetches the job seeker profile from the jobseeker service, and passes that information to the next step.</p>

<p><img class="center" src="/images/denormalize-the-datas-for-great-good/rabbitmq-storm-jobseeker.png" title="The Storm topology extracts the job seeker link from the messages and retrieves information about the job seeker who just applied to the job." ></p>

<p>This third step is responsible for persisting the applicant information to a Couchbase bucket. It uses the job ID as the key, and it does a create or update operation on the document corresponding to that key depending on whether there are applicants already in the bucket for that job.</p>

<p><img class="center" src="/images/denormalize-the-datas-for-great-good/rabbitmq-storm-couchbase.png" title="The final step is that the topology persists the relevant job" ></p>

<p>That last diagram is a bit of a simplification. Although Couchbase is &ldquo;JSON-aware&rdquo;, it lacks the ability to perform certain operations on the JSON documents it stores.  For example, if the document being stored is an Array, and the client&rsquo;s append method is called, we hoped that Couchbase would add that element to the end of the Array. Instead, it&rsquo;s just a blind String.append, resulting in an invalid JSON document. As a result, we had to implement our own append operation by reading the document (if it exists), adding an item to a list if it’s not already there, and then writing the document.  So it’s more like two operations than one.</p>

<p><em>Now</em> when TheLadders mobile service gets a request for Scout information for a job, all it does is a lookup in Couchbase with that job ID and returns the applicants associated with that key.</p>

<p><img class="center" src="/images/denormalize-the-datas-for-great-good/mobile-orchestration-couchbase.png" title="iPhone issues a request for Scout information, backend just retrieves it from couchbase" ></p>

<p><span class='caption-wrapper center'><img class='caption' src='/images/denormalize-the-datas-for-great-good/before-couchbase-after-no-lines.png' width='' height='' alt='95th percentile response time for Scout data, before and after moving to the read view' title='95th percentile response time for Scout data, before and after moving to the read view'><span class='caption-text'>95th percentile response time for Scout data, before and after moving to the read view</span></span></p>

<p>Dramatically faster, even at the 95th percentile.</p>

<hr />

<p>SOA is no panacea. There are many instances where querying a number of backend servers to assemble and aggregate data returned from a database simply doesn&rsquo;t make sense. In those cases, you may do well to denormalize that data and put it in a store that&rsquo;s more efficient for retrieval.</p>

<p>If you find this post interesting, join the dicussion over on <a href="https://news.ycombinator.com/item?id=6015123">Hacker News</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Riders on the Storm: Take a long holiday, Let your children play]]></title>
    <link href="http://dev.theladders.com/2013/03/riders-on-the-storm-take-a-long-holiday-let-your-children-play/"/>
    <updated>2013-03-04T13:52:00-05:00</updated>
    <id>http://dev.theladders.com/2013/03/riders-on-the-storm-take-a-long-holiday-let-your-children-play</id>
    <content type="html"><![CDATA[<p><img class="center" src="/images/lightning_storm.gif" title="&lsquo;Lightning Storm&rsquo;" ></p>

<p><blockquote><p>It was the age of wisdom, it was the age of foolishness</p><footer><strong>&mdash;Charles Dickens</strong></footer></blockquote></p>

<hr />

<h1>Introduction</h1>

<p>I’ve decided to split this blog post up into three different sections as we have gone through three different phases with our usage of Storm.  The first describes how we used to use Storm at TheLadders.  This is followed by our “wake up call”, forcing us to the realization that how we had been using Storm was not sufficient.  The “wake up call” has led us to our current state of how we now use Storm at TheLadders. But first for those of you who aren’t familiar with Storm, a quick explanation straight from the horse’s mouth:</p>

<p><blockquote><p>Storm is a distributed real-time computation system. Similar to how Hadoop provides a set of general primitives for doing batch processing, Storm provides a set of general primitives for doing real-time computation. Storm is simple, can be used with any programming language, is used by many companies, and is a lot of fun to use!</p><footer><strong>Nathan Marz <a href="https://github.com/nathanmarz/storm">https://github.com/nathanmarz/storm</a> storm readme</strong></footer></blockquote></p>

<hr />

<h1>The Past</h1>

<p>We were early users of Storm, starting out with the Storm 0.5.x releases and later upgrading to 0.6.2.  Our Storm cluster was very basic: Nimbus, Zookeeper with 2 Worker nodes;   5 topologies deployed, but only 3 of them really being exercised.  Many of these topologies were for non-critical portions of our application, as such we weren’t paying much attention to the cluster. The topologies we wrote had well-tested individual components; each of the Spouts and Bolts were written with testability in mind.  However we struggled when it came to end-to-end tests for an entire topology.</p>

<p>Other areas of our Storm related pain were:</p>

<ul>
<li>Very limited visibility into the overall health of the Storm cluster.  We lacked any monitoring and had very few metrics regarding our cluster.  We relied a lot on tailing the logs of the worker nodes to see how things were behaving.</li>
<li>We naively configured the topology resources, not really being aware of what resources were being used across the worker nodes.</li>
<li>A majority of our topologies used RabbitMQ as the entry-point and we had a very basic AMQP Spout implementation.  In fact, the initial AMQP Spout increased CPU usage on our RabbitMQ nodes from 4-10% to 40-45% with very little message throughput.</li>
<li>Guaranteed message processing was not always enforced (more due to lack of knowledge on the subject than anything).</li>
</ul>


<p>That list looks bad and one might wonder how we got along at all given those shortcomings.  To be honest, everything just “worked”, which was all we needed at that point.  The combination of Nimbus and Zookeeper did a great job of re-deploying topologies anytime “something” happened.  While we would occasionally open up the Storm web admin to see how things were doing, we really didn’t pay much attention to it; everything just worked.  Even the increase in RabbitMQ CPU usage was not considered overly serious because everything continued to work and was fairly stable.  This behavior continued for about a year or so.</p>

<hr />

<h1>The &ldquo;Wake Up Call&rdquo;</h1>

<p>Then came the day when we needed to deploy a new feature in one of our topologies.  We ran the standard release script to deploy a topology through Nimbus, and … nothing…  After some digging, we found that Nimbus had run out of disk space and our topologies had not been pulling messages off of RabbitMQ for an estimated 3 – 7 days.</p>

<p>In addition, shortly after this initial wake up call, we had some one-off topologies that needed to be run for a 24-hour period.  These topologies required a decent number of resources.  They quickly starved the existing topologies of resources and did a good job of bringing Storm to a screeching halt.  It was like watching a caged death match between all of our topologies that left everyone unconscious on the mat.</p>

<p>If something like the 7-10 day outage can go unnoticed for so long, and if we could starve topologies at the drop of a hat, how could we expect to successfully expand our usage of Storm to more critical portions of the application?</p>

<p>We needed to change, and fast!</p>

<hr />

<h1>The Present</h1>

<p>We immediately started figuring out what we didn’t know about Storm and which bits were the most important for immediate success.  Some members of the development team got together with our operations team and worked out monitoring of cluster components. While Operations beefed up what they could, the development team:</p>

<ul>
<li>Enforced guaranteed message processing in all of our topologies.  This has mainly been done through the use of the BaseBasicBolt, which provides emitting anchored tuples and acking for free. <a href="http://nathanmarz.github.com/storm/doc/backtype/storm/topology/base/BaseBasicBolt.html">(http://nathanmarz.github.com/storm/doc/backtype/storm/topology/base/BaseBasicBolt.html)</a></li>
<li>Refactored our AMQP Spout implementation to subscribe to a queue instead of using individual “gets”.  This has resulted in pre-Storm 0.6.2 levels of CPU usage on our RabbitMQ nodes (below 10% again).</li>
<li>Added an additional three worker nodes with more memory and CPU so we don’t have to constantly worry about resource starvation (although this should always be something to consider really).</li>
<li>Improved our unit testing of configured topologies using the simulated time cluster testing feature in Storm 0.8.1  <a href="https://github.com/xumingming/storm-lib/blob/master/src/jvm/storm/TestingApiDemo.java">(https://github.com/xumingming/storm-lib/blob/master/src/jvm/storm/TestingApiDemo.java)</a>:</li>
</ul>


<hr />

<h1>The Future</h1>

<p>Okay, so I lied.  There is a fourth phase: the future; where we plan to go with Storm.</p>

<p>We plan on upgrading to Storm 0.8.2 very soon.  It has a much-improved web admin that allows for deployment and rebalancing of topologies on the fly.  We hope this simplifies the process of deploying and rebalancing (if needed) of our topologies.</p>

<p>Upgrade to Storm 0.9.x as soon as possible once released.  We hear good things about this release; mainly the metric collecting which will be a huge win in terms of improving visibility into the streams and flow of tuples between Spouts and Bolts.</p>

<p>Finally, we are plan on expanding our topologies to be more than simple queue-to-spout designs.  We are experimenting with using Storm for scheduled batch processes, hoping to have something in production within the next week.</p>

<p>Hopefully this blog gave you a nice overview of how Storm can be used by a company.  I think one of the take-aways can be how easy Storm is to use.  Storm served us for over a year with very little intervention and minimal knowledge on our part; I believe this speaks volumes of Storm’s ease-of-use and reliability.</p>

<p>Stay tuned for some more detailed technical blogs on some of the things we did to improve our Storm usage.</p>

<p>Join the discussion over at <a href="http://www.reddit.com/r/programming/comments/19noko/how_we_use_twitters_storm_part_one/">reddit</a>.</p>
]]></content>
  </entry>
  
</feed>
